#!/usr/bin/env python3
"""
Link Manager for GitHub Pages URLs
Automatically generates and maintains LINK.txt with proper GitHub Pages URLs
for all web-accessible files (.html, .js, .json, .css) in the repository.
"""

import os
import subprocess
import urllib.parse
from pathlib import Path


def get_repo_info():
    """Extract repository owner and name from git remote URL."""
    try:
        result = subprocess.run(
            ['git', 'config', '--get', 'remote.origin.url'],
            capture_output=True,
            text=True,
            check=True
        )
        remote_url = result.stdout.strip()

        # Handle different URL formats
        # SSH: git@github.com:owner/repo.git
        # HTTPS: https://github.com/owner/repo.git
        # Local proxy: http://local_proxy@127.0.0.1:PORT/git/owner/repo

        if 'github.com:' in remote_url:
            # SSH format
            parts = remote_url.split(':')[1].replace('.git', '').split('/')
            owner, repo = parts[0], parts[1]
        elif 'github.com/' in remote_url:
            # HTTPS format
            parts = remote_url.replace('.git', '').split('/')
            owner, repo = parts[-2], parts[-1]
        elif '/git/' in remote_url:
            # Local proxy format
            parts = remote_url.split('/git/')[1].split('/')
            owner, repo = parts[0], parts[1]
        else:
            raise ValueError(f"Unknown git remote URL format: {remote_url}")

        return owner, repo

    except subprocess.CalledProcessError as e:
        print(f"Error: Could not get git remote URL: {e}")
        return None, None
    except Exception as e:
        print(f"Error parsing git remote URL: {e}")
        return None, None


def find_web_files(root_dir='.'):
    """Find all web-accessible files (.html, .js, .json, .css) in the repository."""
    web_files = {
        'html': [],
        'js_clean': [],      # Jsmodules/*.js - readable development versions
        'js_obfuscated': [], # Jsmodules-js/*-js.js - compressed production versions
        'json': [],
        'css': []
    }
    root_path = Path(root_dir).resolve()

    # Find HTML files
    for file_path in root_path.rglob('*.html'):
        rel_path = file_path.relative_to(root_path)
        web_files['html'].append(str(rel_path))

    # Find JavaScript files (categorize by directory)
    for file_path in root_path.rglob('*.js'):
        rel_path = file_path.relative_to(root_path)
        rel_path_str = str(rel_path)

        # Categorize JS files
        if 'Jsmodules-js/' in rel_path_str or '/Jsmodules-js/' in rel_path_str:
            web_files['js_obfuscated'].append(rel_path_str)
        elif 'Jsmodules/' in rel_path_str or '/Jsmodules/' in rel_path_str:
            web_files['js_clean'].append(rel_path_str)
        else:
            # Other JS files go to clean category
            web_files['js_clean'].append(rel_path_str)

    # Find JSON files
    for file_path in root_path.rglob('*.json'):
        rel_path = file_path.relative_to(root_path)
        web_files['json'].append(str(rel_path))

    # Find CSS files
    for file_path in root_path.rglob('*.css'):
        rel_path = file_path.relative_to(root_path)
        web_files['css'].append(str(rel_path))

    # Sort all lists
    for key in web_files:
        web_files[key].sort()

    return web_files


def generate_github_pages_url(owner, repo, file_path):
    """Generate GitHub Pages URL for a given file path."""
    # URL encode the path components properly
    path_parts = file_path.split('/')
    encoded_parts = [urllib.parse.quote(part) for part in path_parts]
    encoded_path = '/'.join(encoded_parts)

    return f"https://{owner}.github.io/{repo}/{encoded_path}"


def load_existing_descriptions(link_file='LINK.txt'):
    """Load existing descriptions from LINK.txt."""
    descriptions = {}

    if not os.path.exists(link_file):
        return descriptions

    try:
        with open(link_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue

                # Split on ' - ' to separate URL from description
                if ' - ' in line:
                    url, description = line.split(' - ', 1)
                    descriptions[url.strip()] = description.strip()

    except Exception as e:
        print(f"Warning: Could not read existing LINK.txt: {e}")

    return descriptions


def update_link_txt(owner, repo, web_files, link_file='LINK.txt'):
    """Update LINK.txt with all web file URLs organized by type."""
    # Load existing descriptions
    existing_descriptions = load_existing_descriptions(link_file)

    # Generate header
    lines = [
        f"# GitHub Pages Links for {owner}/{repo}",
        f"# Auto-generated by PythonHelpers/link_manager.py",
        ""
    ]

    new_entries = 0
    preserved_descriptions = 0

    # HTML Pages Section
    if web_files['html']:
        lines.append("## HTML Pages")
        for file_path in web_files['html']:
            url = generate_github_pages_url(owner, repo, file_path)
            if url in existing_descriptions:
                description = existing_descriptions[url]
                preserved_descriptions += 1
            else:
                filename = os.path.basename(file_path)
                description = f"[Add description for {filename}]"
                new_entries += 1
            lines.append(f"{url} - {description}")
        lines.append("")

    # JavaScript Modules (Clean - Development) Section
    if web_files['js_clean']:
        lines.append("## JavaScript Modules (Clean - Development)")
        for file_path in web_files['js_clean']:
            url = generate_github_pages_url(owner, repo, file_path)
            if url in existing_descriptions:
                description = existing_descriptions[url]
                preserved_descriptions += 1
            else:
                filename = os.path.basename(file_path)
                description = f"[Add description for {filename}]"
                new_entries += 1
            lines.append(f"{url} - {description}")
        lines.append("")

    # JavaScript Modules (Obfuscated - Production) Section
    if web_files['js_obfuscated']:
        lines.append("## JavaScript Modules (Obfuscated - Production)")
        for file_path in web_files['js_obfuscated']:
            url = generate_github_pages_url(owner, repo, file_path)
            if url in existing_descriptions:
                description = existing_descriptions[url]
                preserved_descriptions += 1
            else:
                filename = os.path.basename(file_path)
                description = f"[Add description for {filename}]"
                new_entries += 1
            lines.append(f"{url} - {description}")
        lines.append("")

    # JSON Data Files Section
    if web_files['json']:
        lines.append("## JSON Data Files")
        for file_path in web_files['json']:
            url = generate_github_pages_url(owner, repo, file_path)
            if url in existing_descriptions:
                description = existing_descriptions[url]
                preserved_descriptions += 1
            else:
                filename = os.path.basename(file_path)
                description = f"[Add description for {filename}]"
                new_entries += 1
            lines.append(f"{url} - {description}")
        lines.append("")

    # CSS Files Section
    if web_files['css']:
        lines.append("## CSS Files")
        for file_path in web_files['css']:
            url = generate_github_pages_url(owner, repo, file_path)
            if url in existing_descriptions:
                description = existing_descriptions[url]
                preserved_descriptions += 1
            else:
                filename = os.path.basename(file_path)
                description = f"[Add description for {filename}]"
                new_entries += 1
            lines.append(f"{url} - {description}")
        lines.append("")

    # Write to file
    try:
        with open(link_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

        total_files = sum(len(files) for files in web_files.values())
        print(f"[SUCCESS] Successfully updated {link_file}")
        print(f"  - Total files tracked: {total_files}")
        print(f"    * HTML: {len(web_files['html'])}")
        print(f"    * JS (Clean): {len(web_files['js_clean'])}")
        print(f"    * JS (Obfuscated): {len(web_files['js_obfuscated'])}")
        print(f"    * JSON: {len(web_files['json'])}")
        print(f"    * CSS: {len(web_files['css'])}")
        print(f"  - Preserved {preserved_descriptions} existing descriptions")
        print(f"  - Added {new_entries} new entries")

        return True

    except Exception as e:
        print(f"Error: Could not write to {link_file}: {e}")
        return False


def main():
    """Main function to update LINK.txt."""
    print("Link Manager - GitHub Pages URL Generator")
    print("=" * 50)

    # Get repository info
    print("\n1. Getting repository information...")
    owner, repo = get_repo_info()

    if not owner or not repo:
        print("Error: Could not determine repository owner/name")
        return 1

    print(f"   Repository: {owner}/{repo}")

    # Find web files
    print("\n2. Searching for web-accessible files...")
    web_files = find_web_files()

    total_files = sum(len(files) for files in web_files.values())
    if total_files == 0:
        print("   No web files found in repository")
        return 0

    print(f"   Found {total_files} web files:")
    if web_files['html']:
        print(f"     - HTML: {len(web_files['html'])} files")
    if web_files['js_clean']:
        print(f"     - JavaScript (Clean): {len(web_files['js_clean'])} files")
    if web_files['js_obfuscated']:
        print(f"     - JavaScript (Obfuscated): {len(web_files['js_obfuscated'])} files")
    if web_files['json']:
        print(f"     - JSON: {len(web_files['json'])} files")
    if web_files['css']:
        print(f"     - CSS: {len(web_files['css'])} files")

    # Update LINK.txt
    print("\n3. Updating LINK.txt...")
    success = update_link_txt(owner, repo, web_files)

    if success:
        print("\n[SUCCESS] Link management complete!")
        print("\nNext steps:")
        print("  1. Review LINK.txt for entries marked '[Add description for ...]'")
        print("  2. Update those placeholders with meaningful descriptions")
        print("  3. Commit the changes: git add LINK.txt && git commit -m 'Update LINK.txt'")
        return 0
    else:
        return 1


if __name__ == '__main__':
    exit(main())
